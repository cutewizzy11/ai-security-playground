{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ac040c-8966-4e42-9768-e636449146fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is AI Security?\\n\\nWelcome to the **AI Security Playground**. This notebook gives a beginner-friendly overview of what AI security is, why it matters, and some common examples.\\n\\nYou can read through it like a mini-tutorial and run the code cells as you go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2036f",
   "metadata": {},
   "source": [
    "## 1. Why care about AI security?\\n\\nMachine learning and AI systems are used in many places: email filtering, fraud detection, recommendation systems, chatbots, and more.\\n\\nIf an attacker can fool or control these systems, they can:\\n- Bypass spam filters or fraud detectors\\n- Poison training data to change model behavior\\n- Steal or leak sensitive model information\\n- Abuse LLMs with prompt injection or jailbreaks\\n\\n**AI security** is about understanding and reducing these risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55dd21",
   "metadata": {},
   "source": [
    "## 2. Basic terminology\\n\\nSome words you will see often in AI security:\\n\\n- **Model**: The trained ML/AI system that makes predictions or generates outputs.\\n- **Adversary / Attacker**: Someone trying to intentionally make the model behave badly.\\n- **Threat model**: What we assume the attacker can and cannot do.\\n- **Adversarial example**: An input that has been slightly changed to fool a model.\\n- **Data poisoning**: Injecting malicious data into the training set.\\n- **Evasion attack**: Crafting inputs at inference time to avoid detection.\\n- **LLM prompt injection**: Prompting a language model to ignore or override its original instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea77e0",
   "metadata": {},
   "source": [
    "## 3. A tiny toy example (no real security yet)\\n\\nTo get started, we will train a very small text classifier using scikit-learn. This is **not** a secure model. It is just a simple example of how a model is trained and evaluated. Later projects will explore how such models can be attacked.\\n\\nRun the cell below to import some basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4b2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e1bf5",
   "metadata": {},
   "source": [
    "### 3.1 Create a tiny dataset\\n\\nWe will create a very small dataset of short messages that are either:\\n- Clearly harmless, or\\n- Suspicious / phishing-like.\\n\\nThis is **not** a real dataset, just a simple toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a49aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hey, are we still meeting for lunch today?',\n",
       "  'Reminder: your package will be delivered tomorrow.',\n",
       "  'URGENT: Your account has been compromised, click here to reset your password now!',\n",
       "  'Congratulations, you have won a free prize, click this link!',\n",
       "  'Can you review this document when you have time?',\n",
       "  'Security alert: suspicious login attempt detected on your account.'],\n",
       " array([0, 0, 1, 1, 0, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'Hey, are we still meeting for lunch today?',\n",
    "    'Reminder: your package will be delivered tomorrow.',\n",
    "    'URGENT: Your account has been compromised, click here to reset your password now!',\n",
    "    'Congratulations, you have won a free prize, click this link!',\n",
    "    'Can you review this document when you have time?',\n",
    "    'Security alert: suspicious login attempt detected on your account.',\n",
    "]\n",
    "\n",
    "# 0 = benign, 1 = suspicious / phishing-like\n",
    "labels = np.array([0, 0, 1, 1, 0, 1])\n",
    "\n",
    "texts, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b688a",
   "metadata": {},
   "source": [
    "### 3.2 Train a simple model\\n\\nWe use a **bag-of-words** representation (CountVectorizer) plus logistic regression.\\n\\nThis is a very common baseline for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0b752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test texts: ['URGENT: Your account has been compromised, click here to reset your password now!', 'Can you review this document when you have time?']\n",
      "True labels: [1 0]\n",
      "Predicted labels: [1 1]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts,\n",
    "    labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "model = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    LogisticRegression(max_iter=1000),\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Test texts:', X_test)\n",
    "print('True labels:', y_test)\n",
    "print('Predicted labels:', y_pred)\n",
    "\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3ad8d",
   "metadata": {},
   "source": [
    "### 3.3 Try your own examples\\n\\nNow you can type your own short messages and see what the model predicts.\\n\\nRemember: this model is **very small and fragile**. It is easy to fool and is not suitable for real security use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8c11d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Please update your payment information immediately or your account will be closed.\n",
      "0 - Hi, just checking in about our meeting next week.\n",
      "1 - Click this link to claim your urgent refund.\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    'Please update your payment information immediately or your account will be closed.',\n",
    "    'Hi, just checking in about our meeting next week.',\n",
    "    'Click this link to claim your urgent refund.',\n",
    "]\n",
    "\n",
    "preds = model.predict(examples)\n",
    "for text, label in zip(examples, preds):\n",
    "    print(f'{label} - {text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b34be",
   "metadata": {},
   "source": [
    "## 4. Where does security come in?\\n\\nIn this toy example, attackers might:\\n\\n- Slightly change words to avoid detection (e.g., `cl1ck th1s l1nk`).\\n- Add benign-looking text to hide malicious intent.\\n- Poison the training data with crafted examples so the model learns the wrong patterns.\\n\\nMore advanced attacks include **adversarial examples** for deep learning models and **prompt injection** for LLMs.\\n\\nThe rest of this repository explores these ideas in more detail, with:\\n- **Beginner tracks**: background and simple examples.\\n- **Intermediate tracks**: more realistic models and data.\\n- **Advanced tracks**: adversarial attacks, defenses, and research ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63e75b",
   "metadata": {},
   "source": [
    "## 5. Next steps\\n\\nFrom here, you can:\\n\\n- Explore other notebooks in `tracks/00-intro-to-ai-security`.\\n- Look at mini-projects in `projects/`, starting with the phishing email detector.\\n- Open an issue in the GitHub repo if you have ideas for improving this notebook or adding new ones.\\n\\nWelcome to AI Security Playground!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
