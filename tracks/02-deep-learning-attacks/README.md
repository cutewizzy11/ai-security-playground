# Track: Deep Learning Attacks

This track focuses on attacks and defenses for deep learning models.

## Goals

- Understand what adversarial examples are and how they are crafted.
- Explore simple attack algorithms (e.g., FGSM, PGD) on image models.
- Experiment with basic defenses and robustness ideas.

## Contents (planned)

- Notebooks showing how to generate adversarial images against simple models (e.g., MNIST/CIFAR-like examples).
- Example code for evaluating model robustness.
- Room for more advanced contributions (different attacks, defenses, benchmarks).

Contributions:
- Add notebooks under `notebooks/` demonstrating attacks/defenses.
- Add small helper modules under `src/` if needed.
- Document your experiments and findings in markdown so others can learn from them.
