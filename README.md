# AI Security Playground

A community-driven Python lab for learning and experimenting with AI security:

- Adversarial machine learning
- Phishing / malware / anomaly detection
- LLM security (prompt injection, jailbreaks, etc.)
- Security analytics and threat detection

This repo is open to **beginners, intermediate, and advanced** contributors.

## Quick start

1. Clone the repo
2. Create and activate a virtual environment
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Start Jupyter Lab or Notebook and explore the `tracks/` and `projects/` folders.

## Structure

- `tracks/` – Progressive learning tracks (intro, classical ML security, deep learning attacks, LLM security, research).
- `projects/` – Self-contained mini-projects (e.g., phishing detector, adversarial images, anomaly detection).
- `docs/` – Extra docs for contributors (how-to-contribute, style-guide, roadmap).
- `.github/` – Issue and PR templates.

## How to contribute

- Check open issues, especially those labeled `good first issue`, `beginner`, `intermediate`, or `advanced`.
- Read `CONTRIBUTING.md` for guidelines.
- Open an issue if you want to propose a new track, project, or improvement.

## License

This project is licensed under the terms described in `LICENSE`.
